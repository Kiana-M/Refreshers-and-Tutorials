{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+KHsFjI8ApLvW1pSWYaPD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Toy Project: Titanic Survival Prediction\n",
        "**Objective:** Predict whether a passenger survived or not based on features like age, gender, ticket class, etc.\n",
        "\n",
        "\n",
        "##1. Load the Titanic Dataset:"
      ],
      "metadata": {
        "id": "Gg6vFEdUkLjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-q0jViukJfB",
        "outputId": "3bf14963-94e1-4dbc-9596-86338d3c0a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())  # Inspect the first few rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Exploratory Data Analysis (EDA):"
      ],
      "metadata": {
        "id": "U6VpQaw3l5K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"-------------------------\")\n",
        "#number of duplicates:\n",
        "print(len(df['PassengerId'])-len(df['PassengerId'].drop_duplicates()))\n",
        "\n",
        "print(\"-------------------------\")\n",
        "# Descriptive statistics\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvgBDchVl8qT",
        "outputId": "a78fa34b-2b15-44d6-9788-9b28f8e66e3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n",
            "-------------------------\n",
            "0\n",
            "-------------------------\n",
            "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
            "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
            "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
            "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
            "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
            "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
            "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
            "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
            "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
            "\n",
            "            Parch        Fare  \n",
            "count  891.000000  891.000000  \n",
            "mean     0.381594   32.204208  \n",
            "std      0.806057   49.693429  \n",
            "min      0.000000    0.000000  \n",
            "25%      0.000000    7.910400  \n",
            "50%      0.000000   14.454200  \n",
            "75%      0.000000   31.000000  \n",
            "max      6.000000  512.329200  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Data Cleaning & Preprocessing:"
      ],
      "metadata": {
        "id": "FO_NBexPfX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "# Fill missing values\n",
        "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "lLQe0-PNfbcI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Feature Engineering & Scaling:"
      ],
      "metadata": {
        "id": "4yhyN7wv9OQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale 'Age' and 'Fare' columns\n",
        "scaler = StandardScaler()\n",
        "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n"
      ],
      "metadata": {
        "id": "I8qT3tSp9PPJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are not scaling 'SibSb' and 'Parch' because they are small values, but if you have discrete values that need to normalized in your data, you can use MinMaxScaler instead of fit_transform.\n",
        "from sklearn.preprocessing import MinMaxScaler.\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assume 'SibSp' and 'Parch' are discrete numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[['SibSp', 'Parch']] = scaler.fit_transform(df[['SibSp', 'Parch']])\n",
        "\n",
        "# Scaled 'SibSp' and 'Parch' will now be between 0 and 1\n",
        "```\n",
        "##General Guidance:\n",
        "\n",
        "- If you are using algorithms that are sensitive to the scale of input data (e.g., logistic regression, SVM, KNN, neural networks), scaling with StandardScaler or MinMaxScaler is usually a good idea, even for non-continuous numerical values.\n",
        "- For models like tree-based algorithms (e.g., decision trees, random forests, gradient boosting), you can often leave non-continuous numerical features as they are because these models are not sensitive to the scale of the data."
      ],
      "metadata": {
        "id": "qToWxjcQ_j8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Train-Test Split:"
      ],
      "metadata": {
        "id": "d6vrlKRlAZ5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "p3qYgmLzAgWn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Modeling:\n",
        "Logistic Regression:"
      ],
      "metadata": {
        "id": "ovtn8bmWAl-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_log = log_reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "p0IxelMgApcS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree:"
      ],
      "metadata": {
        "id": "g-HIXfzYAusy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train the model\n",
        "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_tree = tree_clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "2zrX_NcgA0Nk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation:"
      ],
      "metadata": {
        "id": "bsozNBU3A3Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Logistic Regression performance\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(confusion_matrix(y_test, y_pred_log))\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "\n",
        "# Decision Tree performance\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "print(confusion_matrix(y_test, y_pred_tree))\n",
        "print(classification_report(y_test, y_pred_tree))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk0AeyfjA_uw",
        "outputId": "fbf92bb9-aa62-4708-9cd6-e8751a0de602"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8100558659217877\n",
            "[[90 15]\n",
            " [19 55]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "Decision Tree Accuracy: 0.7988826815642458\n",
            "[[92 13]\n",
            " [23 51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       105\n",
            "           1       0.80      0.69      0.74        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.78      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Cross-Validation for Model Evaluation:"
      ],
      "metadata": {
        "id": "EPQ3xtX0BKq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validation for Logistic Regression\n",
        "cv_scores_log = cross_val_score(log_reg, X, y, cv=5)\n",
        "print(\"Logistic Regression CV Scores:\", cv_scores_log)\n",
        "\n",
        "# Cross-validation for Decision Tree\n",
        "cv_scores_tree = cross_val_score(tree_clf, X, y, cv=5)\n",
        "print(\"Decision Tree CV Scores:\", cv_scores_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd4IBEVWBPvP",
        "outputId": "9bd5112a-001e-4ab5-e930-ddeb92c0f4e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression CV Scores: [0.78212291 0.78089888 0.78651685 0.76966292 0.8258427 ]\n",
            "Decision Tree CV Scores: [0.81564246 0.81460674 0.81460674 0.78089888 0.82022472]\n"
          ]
        }
      ]
    }
  ]
}